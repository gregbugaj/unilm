diff --git a/layoutlmv3/README-GB.md b/layoutlmv3/README-GB.md
index bd4f519..4e871cf 100644
--- a/layoutlmv3/README-GB.md
+++ b/layoutlmv3/README-GB.md
@@ -21,11 +21,17 @@ python -m torch.distributed.launch --nproc_per_node=1 ./train.py
 ``` 
 
 
-
 ```
 tensorboard --logdir=./logs
 ```
 
+```
+wandb server start --upgrade
+```
+
+```
+wandb login --relogin
+```
 
 
 LARGE
@@ -248,6 +254,30 @@ LARGE
 
 
 
+
+wandb: Run summary:
+wandb:                  eval/accuracy 0.89267
+wandb:                        eval/f1 0.84425
+wandb:                      eval/loss 1.22023
+wandb:                 eval/precision 0.83448
+wandb:                    eval/recall 0.85426
+wandb:                   eval/runtime 8.3803
+wandb:        eval/samples_per_second 47.134
+wandb:          eval/steps_per_second 47.134
+wandb:                    train/epoch 50.25
+wandb:              train/global_step 10000
+wandb:            train/learning_rate 0.0
+wandb:                     train/loss 0.0001
+wandb:               train/total_flos 1.0610876067072e+16
+wandb:               train/train_loss 0.04689
+wandb:            train/train_runtime 1368.8808
+wandb: train/train_samples_per_second 29.221
+wandb:   train/train_steps_per_second 7.305
+
+
+
+
+
 # Reference 
 
 [Max_seq_length LayoutLMV3 - How to implement it ? #942](https://github.com/microsoft/unilm/issues/942)
diff --git a/layoutlmv3/examples/funsd_dataset/funsd_dataset.py b/layoutlmv3/examples/funsd_dataset/funsd_dataset.py
index 6b250a5..50ded28 100644
--- a/layoutlmv3/examples/funsd_dataset/funsd_dataset.py
+++ b/layoutlmv3/examples/funsd_dataset/funsd_dataset.py
@@ -153,6 +153,8 @@ class Funsd(datasets.GeneratorBasedBuilder):
         downloaded_file = "/data/dataset/private/corr-indexer-augmented"
         # downloaded_file = "/home/greg/dataset/assets-private/corr-indexer-augmented"
         downloaded_file = "/home/gbugaj/datasets/private/corr-indexer-augmented"
+        downloaded_file = "/home/greg/datasets/private/assets-private/corr-indexer-augmented"
+        # downloaded_file = "/home/gbugaj/dataset/private/corr-indexer-augmented"
 
         return [
             datasets.SplitGenerator(
@@ -220,7 +222,10 @@ class Funsd(datasets.GeneratorBasedBuilder):
                     tokens.append(w["text"])
                     ner_tags.append("I-" + label.upper())
                     cur_line_bboxes.append(normalize_bbox(w["box"], size))
-            # cur_line_bboxes = self.get_line_bbox(cur_line_bboxes)
+
+                # by default: --segment_level_layout 1
+                # if do not want to use segment_level_layout, comment the following line
+                cur_line_bboxes = self.get_line_bbox(cur_line_bboxes)
 
             if False:
                 boxed = True
diff --git a/layoutlmv3/examples/train.py b/layoutlmv3/examples/train.py
index c41ad25..c4bec5e 100644
--- a/layoutlmv3/examples/train.py
+++ b/layoutlmv3/examples/train.py
@@ -80,6 +80,7 @@ if False:
 # processor = AutoProcessor.from_pretrained("microsoft/layoutlmv3-base", apply_ocr=False)
 
 model_name_or_path = "microsoft/layoutlmv3-large"
+model_name_or_path = "microsoft/layoutlmv3-base"
 
 config = AutoConfig.from_pretrained (
     model_name_or_path,
@@ -87,9 +88,9 @@ config = AutoConfig.from_pretrained (
     finetuning_task="ner",
     cache_dir="/mnt/data/cache",
     input_size=224,
-    hidden_dropout_prob = .2,
-    attention_probs_dropout_prob = .2,
-    has_relative_attention_bias=False
+    # hidden_dropout_prob = .1,
+    # attention_probs_dropout_prob = .1,
+    # has_relative_attention_bias=False
 )
 
 # config.hidden_dropout_prob = 0.2
@@ -239,7 +240,7 @@ training_args = TrainingArguments(
                   eval_steps=1000,
                   load_best_model_at_end=True,
                   metric_for_best_model="f1",
-                  output_dir="/mnt/data/models/layoutlmv3-large-fullyannotated",
+                  output_dir="/mnt/data/models/layoutlmv3-base-segment_level_layout",
                   # resume_from_checkpoint="/mnt/data/models/layoutlmv3-large-finetuned-small-100/checkpoint-750",
                   fp16=True,
                 )
