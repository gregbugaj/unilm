diff --git a/layoutlmv3/README-GB.md b/layoutlmv3/README-GB.md
index bd4f519..d2f16fa 100644
--- a/layoutlmv3/README-GB.md
+++ b/layoutlmv3/README-GB.md
@@ -21,11 +21,17 @@ python -m torch.distributed.launch --nproc_per_node=1 ./train.py
 ``` 
 
 
-
 ```
 tensorboard --logdir=./logs
 ```
 
+```
+wandb server start --upgrade
+```
+
+```
+wandb login --relogin
+```
 
 
 LARGE
@@ -248,6 +254,114 @@ LARGE
 
 
 
+-- BASE 
+wandb: Run summary:
+wandb:                  eval/accuracy 0.89267
+wandb:                        eval/f1 0.84425
+wandb:                      eval/loss 1.22023
+wandb:                 eval/precision 0.83448
+wandb:                    eval/recall 0.85426
+wandb:                   eval/runtime 8.3803
+wandb:        eval/samples_per_second 47.134
+wandb:          eval/steps_per_second 47.134
+wandb:                    train/epoch 50.25
+wandb:              train/global_step 10000
+wandb:            train/learning_rate 0.0
+wandb:                     train/loss 0.0001
+wandb:               train/total_flos 1.0610876067072e+16
+wandb:               train/train_loss 0.04689
+wandb:            train/train_runtime 1368.8808
+wandb: train/train_samples_per_second 29.221
+wandb:   train/train_steps_per_second 7.305
+
+
+
+--- BASE SEGMENT_CURRENT_LINE
+
+wandb: 
+wandb: Run history:
+wandb:                  eval/accuracy ▅▄▁▄▇▆▆▅██▇
+wandb:                        eval/f1 ▄▅▁▄█▆▄▃▅▅█
+wandb:                      eval/loss ▁▄▅▆▇▅▆▇▇█▇
+wandb:                 eval/precision ▄▅▁▅█▆▃▁▄▅█
+wandb:                    eval/recall ▄▅▁▄█▇▆▅▆▆█
+wandb:                   eval/runtime ▆█▇▁▅▆▃▃▅▄▄
+wandb:        eval/samples_per_second ▂▁▂█▃▃▅▅▃▅▅
+wandb:          eval/steps_per_second ▂▁▂█▃▃▅▅▃▅▅
+wandb:                    train/epoch ▁▁▁▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇█████
+wandb:              train/global_step ▁▁▁▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇█████
+wandb:            train/learning_rate ██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁
+wandb:                     train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
+wandb:               train/total_flos ▁
+wandb:               train/train_loss ▁
+wandb:            train/train_runtime ▁
+wandb: train/train_samples_per_second ▁
+wandb:   train/train_steps_per_second ▁
+wandb: 
+wandb: Run summary:
+wandb:                  eval/accuracy 0.9023
+wandb:                        eval/f1 0.86765
+wandb:                      eval/loss 1.1141
+wandb:                 eval/precision 0.85966
+wandb:                    eval/recall 0.87579
+wandb:                   eval/runtime 9.6308
+wandb:        eval/samples_per_second 41.014
+wandb:          eval/steps_per_second 41.014
+wandb:                    train/epoch 50.25
+wandb:              train/global_step 10000
+wandb:            train/learning_rate 0.0
+wandb:                     train/loss 0.0001
+wandb:               train/total_flos 1.0610876067072e+16
+wandb:               train/train_loss 0.0379
+wandb:            train/train_runtime 1663.565
+wandb: train/train_samples_per_second 24.045
+wandb:   train/train_steps_per_second 6.011
+
+
+-- BASE  STRIDE 128
+
+wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
+wandb: 
+wandb: Run history:
+wandb:                  eval/accuracy ▁▆▆▅▅█▇▇▇▇▇
+wandb:                        eval/f1 ▁▆▆▆▇▇█▇███
+wandb:                      eval/loss ▁▁▃▅▇▅▆▆▇█▆
+wandb:                 eval/precision ▁▆▅▆▇▇█▇███
+wandb:                    eval/recall ▁▇▇▆▇▇█████
+wandb:                   eval/runtime ▅▅▇▄█▄▁▃▁▁▂
+wandb:        eval/samples_per_second ▄▄▂▄▁▅█▆██▇
+wandb:          eval/steps_per_second ▄▄▂▄▁▅█▆██▇
+wandb:                    train/epoch ▁▁▁▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇█████
+wandb:              train/global_step ▁▁▁▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇█████
+wandb:            train/learning_rate ██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁
+wandb:                     train/loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
+wandb:               train/total_flos ▁
+wandb:               train/train_loss ▁
+wandb:            train/train_runtime ▁
+wandb: train/train_samples_per_second ▁
+wandb:   train/train_steps_per_second ▁
+wandb: 
+wandb: Run summary:
+wandb:                  eval/accuracy 0.91079
+wandb:                        eval/f1 0.85973
+wandb:                      eval/loss 0.91185
+wandb:                 eval/precision 0.84622
+wandb:                    eval/recall 0.87368
+wandb:                   eval/runtime 13.3387
+wandb:        eval/samples_per_second 46.256
+wandb:          eval/steps_per_second 46.256
+wandb:                    train/epoch 30.3
+wandb:              train/global_step 10000
+wandb:            train/learning_rate 0.0
+wandb:                     train/loss 0.0001
+wandb:               train/total_flos 1.06002519108096e+16
+wandb:               train/train_loss 0.04648
+wandb:            train/train_runtime 1544.4472
+wandb: train/train_samples_per_second 25.899
+wandb:   train/train_steps_per_second 6.475
+
+
+
 # Reference 
 
 [Max_seq_length LayoutLMV3 - How to implement it ? #942](https://github.com/microsoft/unilm/issues/942)
diff --git a/layoutlmv3/examples/funsd_dataset/funsd_dataset.py b/layoutlmv3/examples/funsd_dataset/funsd_dataset.py
index 6b250a5..28075d3 100644
--- a/layoutlmv3/examples/funsd_dataset/funsd_dataset.py
+++ b/layoutlmv3/examples/funsd_dataset/funsd_dataset.py
@@ -153,6 +153,8 @@ class Funsd(datasets.GeneratorBasedBuilder):
         downloaded_file = "/data/dataset/private/corr-indexer-augmented"
         # downloaded_file = "/home/greg/dataset/assets-private/corr-indexer-augmented"
         downloaded_file = "/home/gbugaj/datasets/private/corr-indexer-augmented"
+        downloaded_file = "/home/greg/datasets/private/assets-private/corr-indexer-augmented"
+        # downloaded_file = "/home/gbugaj/dataset/private/corr-indexer-augmented"
 
         return [
             datasets.SplitGenerator(
@@ -179,7 +181,6 @@ class Funsd(datasets.GeneratorBasedBuilder):
         ann_dir = os.path.join(self.filepath, "annotations")
         img_dir = os.path.join(self.filepath, "images")
 
-        print(guid)
         tokens = []
         bboxes = []
         ner_tags = []
@@ -191,6 +192,22 @@ class Funsd(datasets.GeneratorBasedBuilder):
         image_path = os.path.join(img_dir, file)
         image_path = image_path.replace("json", "png")
         image, size = load_image(image_path)
+
+        # somehow we got a TEXT token with size of [0,0,W,H]
+        # TODO: investigate
+        # example :/corr-indexer-augmented/dataset/training_data/annotations/152658473_2_140_0.json
+        #       "words": [
+        # {
+        #   "box": [
+        #     0,
+        #     0,
+        #     776,
+        #     1000
+        #   ],
+        #   "text": ":"
+        # }
+        # ]
+    
         for item in data["form"]:
             cur_line_bboxes = []
             words, label = item["words"], item["label"]
@@ -202,6 +219,7 @@ class Funsd(datasets.GeneratorBasedBuilder):
             words = [w for w in words if w["text"].strip() != ""]
             if len(words) == 0:
                 continue
+
             if label == "other":
                 for w in words:
                     # TODO: How did we endup with O-Token with size of [0,0,W,H]
@@ -220,26 +238,20 @@ class Funsd(datasets.GeneratorBasedBuilder):
                     tokens.append(w["text"])
                     ner_tags.append("I-" + label.upper())
                     cur_line_bboxes.append(normalize_bbox(w["box"], size))
-            # cur_line_bboxes = self.get_line_bbox(cur_line_bboxes)
-
-            if False:
-                boxed = True
-                label = label.upper()
-                if label == "OTHER" or label== "ANSWER" or label == "PARAGRAPH" or label == "ADDRESS" or label == "GREETING" or label == "HEADER" :
-                    boxed = False
-
-                if boxed:
-                    # print(f"B {label} : {words} >> {cur_line_bboxes}")
-                    cur_line_bboxes = self.get_line_bbox(cur_line_bboxes)
-                    pass
-                    
+
+            # by default: --segment_level_layout 1
+            # if do not want to use segment_level_layout, comment the following line
+            if len(cur_line_bboxes) == 0:
+                print(f"Empty cur_line_bboxes for {words} : {file_path}")
+                continue
+                
+            cur_line_bboxes = self.get_line_bbox(cur_line_bboxes)
             bboxes.extend(cur_line_bboxes)
 
 
         if len(bboxes) == 0:
             payload = {"id": str(guid), "tokens": tokens, "bboxes": bboxes, "ner_tags": ner_tags}
             print(f"Empty Boxes for : {file_path}")
-            # print(payload)
             return 
 
         return guid, {"id": str(guid), "tokens": tokens, "bboxes": bboxes, "ner_tags": ner_tags,
diff --git a/layoutlmv3/examples/run_funsd.py b/layoutlmv3/examples/run_funsd.py
index 12c005e..844f842 100644
--- a/layoutlmv3/examples/run_funsd.py
+++ b/layoutlmv3/examples/run_funsd.py
@@ -339,6 +339,7 @@ def main():
         boxes = examples[boxes_column_name]
         word_labels = examples[label_column_name]
 
+        # is_split_into_words = True   is_pretokenized = True
         tokenized_inputs = tokenizer(
             words,
             boxes=boxes, 
@@ -347,7 +348,7 @@ def main():
             truncation=True,
             return_overflowing_tokens=True,
             # We use this argument because the texts in our dataset are lists of words (with a label for each word).
-            # is_split_into_words=True, TODO : This is not working, possible bug in Tokenizer
+            # is_split_into_words=True, #TODO : This is not working, possible bug in Tokenizer
         )
 
         labels = []
diff --git a/layoutlmv3/examples/train.py b/layoutlmv3/examples/train.py
index c41ad25..80a3ff9 100644
--- a/layoutlmv3/examples/train.py
+++ b/layoutlmv3/examples/train.py
@@ -80,6 +80,7 @@ if False:
 # processor = AutoProcessor.from_pretrained("microsoft/layoutlmv3-base", apply_ocr=False)
 
 model_name_or_path = "microsoft/layoutlmv3-large"
+# model_name_or_path = "microsoft/layoutlmv3-base"
 
 config = AutoConfig.from_pretrained (
     model_name_or_path,
@@ -87,15 +88,11 @@ config = AutoConfig.from_pretrained (
     finetuning_task="ner",
     cache_dir="/mnt/data/cache",
     input_size=224,
-    hidden_dropout_prob = .2,
-    attention_probs_dropout_prob = .2,
-    has_relative_attention_bias=False
+    # hidden_dropout_prob = .1,
+    # attention_probs_dropout_prob = .1,
+    # has_relative_attention_bias=False
 )
 
-# config.hidden_dropout_prob = 0.2
-# config.attention_probs_dropout_prob = 0.1
-
-
 # Max model size is 512, so we will need to handle any documents larger thank that
 feature_extractor = LayoutLMv3FeatureExtractor(apply_ocr=False, do_resize=True, resample=Image.LANCZOS)
 tokenizer = LayoutLMv3TokenizerFast.from_pretrained(model_name_or_path)
@@ -140,7 +137,14 @@ def prepare_examples(examples):
   boxes = examples[boxes_column_name]
   word_labels = examples[label_column_name]
 
-  encoding = processor(images, words, boxes=boxes, word_labels=word_labels, truncation=True,  padding="max_length")
+#   encoding = processor(images, words, boxes=boxes, word_labels=word_labels, truncation=True,  padding="max_length")
+
+  encoding = processor(images, words, boxes=boxes, word_labels=word_labels, truncation=True, stride =128, 
+         padding="max_length", max_length=512, return_overflowing_tokens=True, return_offsets_mapping=True)  
+  
+  offset_mapping = encoding.pop('offset_mapping')
+  overflow_to_sample_mapping = encoding.pop('overflow_to_sample_mapping')
+  
   return encoding
 
 # we need to define custom features for `set_format` (used later on) to work properly
@@ -239,8 +243,8 @@ training_args = TrainingArguments(
                   eval_steps=1000,
                   load_best_model_at_end=True,
                   metric_for_best_model="f1",
-                  output_dir="/mnt/data/models/layoutlmv3-large-fullyannotated",
-                  # resume_from_checkpoint="/mnt/data/models/layoutlmv3-large-finetuned-small-100/checkpoint-750",
+                  output_dir="/mnt/data/models/layoutlmv3-base-stride",
+                  resume_from_checkpoint="/mnt/data/models/layoutlmv3-base-stride/checkpoint-6000",
                   fp16=True,
                 )
 
@@ -253,7 +257,7 @@ trainer = Trainer(
     args=training_args,
     train_dataset=train_dataset,
     eval_dataset=eval_dataset,
-    tokenizer=processor,
+    tokenizer=tokenizer,
     data_collator=default_data_collator,
     compute_metrics=compute_metrics,
 )
@@ -405,26 +409,3 @@ for word, box, label in zip(example['tokens'], example['bboxes'], example['ner_t
 image.save("/tmp/tensors/real.png")
 
 
-#
-#
-# Loading best model from /mnt/data/models/layoutlmv3-large-finetuned-small/checkpoint-4500 (score: 0.9116538131962296).
-# {'train_runtime': 2593.4786, 'train_samples_per_second': 7.712, 'train_steps_per_second': 1.928, 'train_loss': 0.06096109193563461, 'epoch': 18.73}
-# 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [43:13<00:00,  1.93it/s]
-# Saving model checkpoint to /mnt/data/models/layoutlmv3-large-finetuned-small
-# Configuration saved in /mnt/data/models/layoutlmv3-large-finetuned-small/config.json
-# Model weights saved in /mnt/data/models/layoutlmv3-large-finetuned-small/pytorch_model.bin
-# Feature extractor saved in /mnt/data/models/layoutlmv3-large-finetuned-small/preprocessor_config.json
-# tokenizer config file saved in /mnt/data/models/layoutlmv3-large-finetuned-small/tokenizer_config.json
-# Special tokens file saved in /mnt/data/models/layoutlmv3-large-finetuned-small/special_tokens_map.json
-# ***** train metrics *****
-#   epoch                    =      18.73
-#   train_loss               =      0.061
-#   train_runtime            = 0:43:13.47
-#   train_samples            =       1065
-#   train_samples_per_second =      7.712
-#   train_steps_per_second   =      1.928
-# ***** Running Evaluation *****
-#   Num examples = 400
-#   Batch size = 1
-
-
diff --git a/layoutlmv3/layoutlmft/data/funsd.py b/layoutlmv3/layoutlmft/data/funsd.py
index f2478ae..da616c9 100644
--- a/layoutlmv3/layoutlmft/data/funsd.py
+++ b/layoutlmv3/layoutlmft/data/funsd.py
@@ -4,12 +4,11 @@ Reference: https://huggingface.co/datasets/nielsr/funsd/blob/main/funsd.py
 '''
 import json
 import os
-
-import datasets
-
 from re import L
 
 from PIL import Image
+
+import datasets
 import multiprocessing as mp
 from concurrent.futures.thread import ThreadPoolExecutor
 import concurrent.futures
@@ -18,20 +17,41 @@ from multiprocessing import Pool
 
 from PIL import Image, ImageDraw, ImageFont
 import numpy as np
+
 import torch
 
+def load_image(image_path):
+    if not os.path.exists(image_path):
+        raise FileNotFoundError(image_path)
 
-from layoutlmft.data.image_utils import load_image, normalize_bbox
+    image = Image.open(image_path).convert("RGB")
+    w, h = image.size
+    return image, (w, h)
 
+def normalize_bbox(bbox, size):
+    return [
+        int(1000 * bbox[0] / size[0]),
+        int(1000 * bbox[1] / size[1]),
+        int(1000 * bbox[2] / size[0]),
+        int(1000 * bbox[3] / size[1]),
+    ]
 
 logger = datasets.logging.get_logger(__name__)
 
 
 _CITATION = """\
-
+@article{Jaume2019FUNSDAD,
+  title={FUNSD: A Dataset for Form Understanding in Noisy Scanned Documents},
+  author={Guillaume Jaume and H. K. Ekenel and J. Thiran},
+  journal={2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)},
+  year={2019},
+  volume={2},
+  pages={1-6}
+}
 """
 
 _DESCRIPTION = """\
+https://guillaumejaume.github.io/FUNSD/
 """
 
 
@@ -48,15 +68,10 @@ class FunsdConfig(datasets.BuilderConfig):
 
 
 class Funsd(datasets.GeneratorBasedBuilder):
-    """dataset."""
-
-    # 1.10.1 -> LV3 single box layout (GOOD)
-    # 1.10.2 -> LV3 mulibox layout (????)
-    # 1.10.3 -> LV3 mulibox layout / small dataset
-    
+    """Conll2003 dataset."""
 
     BUILDER_CONFIGS = [
-        FunsdConfig(name="funsd", version=datasets.Version("1.10.3"), description="FUNSD Like dataset"),
+        FunsdConfig(name="funsd", version=datasets.Version("1.8.0"), description="FUNSD like dataset, corr-indexing"),
     ]
 
     def _info(self):
@@ -73,9 +88,9 @@ class Funsd(datasets.GeneratorBasedBuilder):
                     #     )
                     # ),
 
-                     "ner_tags": datasets.Sequence(
+                    "ner_tags": datasets.Sequence(
                         datasets.features.ClassLabel(
-                             names=[
+                            names=[
                                 "O",
                                 'B-MEMBER_NAME', 'I-MEMBER_NAME',
                                 'B-MEMBER_NUMBER', 'I-MEMBER_NUMBER',
@@ -109,16 +124,27 @@ class Funsd(datasets.GeneratorBasedBuilder):
                                 'B-CHECK_AMT_ANSWER', 'I-CHECK_AMT_ANSWER',
                                 'B-CHECK_NUMBER', 'I-CHECK_NUMBER',
                                 'B-CHECK_NUMBER_ANSWER', 'I-CHECK_NUMBER_ANSWER',
+                                'B-LIST', 'I-LIST',
+                                'B-FOOTER', 'I-FOOTER',
+                                'B-DATE', 'I-DATE',
+                                'B-IDENTIFIER', 'I-IDENTIFIER',
+                                'B-PROC_CODE', 'I-PROC_CODE',
+                                'B-PROC_CODE_ANSWER', 'I-PROC_CODE_ANSWER',
+                                'B-PROVIDER', 'I-PROVIDER',
+                                'B-PROVIDER_ANSWER', 'I-PROVIDER_ANSWER',
+                                'B-MONEY', 'I-MONEY',
+                                'B-COMPANY', 'I-COMPANY',
                             ]
                         )
                     ),
-                    
-                    "image": datasets.Array3D(shape=(3, 224, 224), dtype="uint8"),
+
+                    "image": datasets.features.Image(),
+                    # "image": datasets.Array3D(shape=(3, 224, 224), dtype="uint8"),
                     "image_path": datasets.Value("string"),
                 }
             ),
             supervised_keys=None,
-            homepage="",
+            homepage="https://guillaumejaume.github.io/FUNSD/",
             citation=_CITATION,
         )
 
@@ -127,6 +153,10 @@ class Funsd(datasets.GeneratorBasedBuilder):
         # downloaded_file = dl_manager.download_and_extract("https://guillaumejaume.github.io/FUNSD/dataset.zip")
 
         downloaded_file = "/data/dataset/private/corr-indexer-augmented"
+        # downloaded_file = "/home/greg/dataset/assets-private/corr-indexer-augmented"
+        downloaded_file = "/home/gbugaj/datasets/private/corr-indexer-augmented"
+        downloaded_file = "/home/greg/datasets/private/assets-private/corr-indexer-augmented"
+        # downloaded_file = "/home/gbugaj/dataset/private/corr-indexer-augmented"
 
         return [
             datasets.SplitGenerator(
@@ -137,6 +167,7 @@ class Funsd(datasets.GeneratorBasedBuilder):
             ),
         ]
 
+
     def get_line_bbox(self, bboxs):
         x = [bboxs[i][j] for i in range(len(bboxs)) for j in range(0, len(bboxs[i]), 2)]
         y = [bboxs[i][j] for i in range(len(bboxs)) for j in range(1, len(bboxs[i]), 2)]
@@ -147,122 +178,89 @@ class Funsd(datasets.GeneratorBasedBuilder):
         bbox = [[x0, y0, x1, y1] for _ in range(len(bboxs))]
         return bbox
 
-
-    def _generate_examples(self, filepath):
-        logger.info("⏳ Generating examples from = %s", filepath)
-        ann_dir = os.path.join(filepath, "annotations")
-        img_dir = os.path.join(filepath, "images")
-
-        items = sorted(os.listdir(ann_dir))
-        np.random.shuffle(items)
-
-        stop = int(len(items) *.25)
-        # stop = int(len(items))
-        print(f"stop = {stop}")
-        font = ImageFont.load_default()
-
-        for guid, file in enumerate(items):
-            if guid == stop:
-                break
-
-            print(f"{guid} : {file}")
-            tokens = []
-            bboxes = []
-            ner_tags = []
-            bboxes_raw = []
-
-            file_path = os.path.join(ann_dir, file)
-            with open(file_path, "r", encoding="utf8") as f:
-                data = json.load(f)
-            image_path = os.path.join(img_dir, file)
-            image_path = image_path.replace("json", "png")
-            image, size = load_image(image_path)
-            # image_pil = Image.open(image_path).convert("RGB")
-            word_index = 0
-            for item in data["form"]:
-                cur_line_bboxes = []
-                words, label = item["words"], item["label"]
-
-                # remap bad 'text:' label with `:`                
+    def _generate_(self, guid, file):
+        logger.info("⏳ Generating examples from = %s", self.filepath)
+        ann_dir = os.path.join(self.filepath, "annotations")
+        img_dir = os.path.join(self.filepath, "images")
+
+        tokens = []
+        bboxes = []
+        ner_tags = []
+
+
+        file_path = os.path.join(ann_dir, file)
+        with open(file_path, "r", encoding="utf8") as f:
+            data = json.load(f)
+        image_path = os.path.join(img_dir, file)
+        image_path = image_path.replace("json", "png")
+        image, size = load_image(image_path)
+
+        # somehow we got a TEXT token with size of [0,0,W,H]
+        # TODO: investigate
+        # example :/corr-indexer-augmented/dataset/training_data/annotations/152658473_2_140_0.json
+        #       "words": [
+        # {
+        #   "box": [
+        #     0,
+        #     0,
+        #     776,
+        #     1000
+        #   ],
+        #   "text": ":"
+        # }
+        # ]
+    
+        for item in data["form"]:
+            cur_line_bboxes = []
+            words, label = item["words"], item["label"]
+            # remap bad 'text:' label with `:`
+            for w in words:
+                if "text:" in w:
+                    w["text"] = w["text:"]
+
+            words = [w for w in words if w["text"].strip() != ""]
+            if len(words) == 0:
+                continue
+
+            if label == "other":
                 for w in words:
-                    if "text:" in w:
-                        w["text"] = w["text:"]
-
-                for w in words :
-                    if "text" not in w:
-                        print(w)
-                        raise Exception("EX")
-
-                words = [w for w in words if w["text"].strip() != ""]
-
-                if len(words) == 0:
-                    continue
-                if label == "other":
-                    for w in words:
-                        # TODO: How did we endup with O-Token with size of [0,0,W,H]
-                        other_box  = normalize_bbox(w["box"], size)
-                        if other_box[0]== 0 and other_box[1] == 0:
-                            continue
-                        tokens.append(w["text"])
-                        ner_tags.append("O")
-                        cur_line_bboxes.append(other_box)
-                        bboxes_raw.append(w["box"])
-                else:
-                    tokens.append(words[0]["text"])
-                    ner_tags.append("B-" + label.upper())
-                    cur_line_bboxes.append(normalize_bbox(words[0]["box"], size))
-                    bboxes_raw.append(words[0]["box"])
-
-                    for w in words[1:]:
-                        tokens.append(w["text"])
-                        ner_tags.append("I-" + label.upper())
-                        cur_line_bboxes.append(normalize_bbox(w["box"], size))
-                        bboxes_raw.append(w["box"])
-                # by default: --segment_level_layout 1
-                # if do not want to use segment_level_layout, comment the following line
-                # cur_line_bboxes = self.get_line_bbox(cur_line_bboxes)
-
-                if True:
-                    boxed = True
-                    label = label.upper()
-                    if label == "OTHER" or label== "ANSWER" or label == "PARAGRAPH" or label == "ADDRESS" or label == "GREETING" or label == "HEADER" :
-                        boxed = False
-
-                    if boxed:
-                        # print(f"B {label} : {words} >> {cur_line_bboxes}")
-                        cur_line_bboxes = self.get_line_bbox(cur_line_bboxes)
-                        # pass
-                        
-                bboxes.extend(cur_line_bboxes)
-
-            if False:
-                draw = ImageDraw.Draw(image_pil)
-                # print(f"B = {bboxes_raw}")
-                bboxes_raw = self.get_line_bbox(bboxes) 
-                # print(f"A = {bboxes_raw}")
-                for tag, box in zip(ner_tags, bboxes):
-                    print(f"TAG = {tag} : {box}")
-                    text = f"{word_index}"
-                    draw.rectangle(box, outline='red')
-                    draw.text((box[0] + 10, box[1] - 10), text=text, fill='blue' )
-                    # print(f"\t\t\t {word_index} => {tag} -> {words_lower}")
-                    word_index +=1
-                image_pil.save(f"/tmp/snippet/{guid}.png")
-                # os.exit()
-        
-        if len(bboxes) != len(tokens):
-            print("ASSERTION ERROR")
-            print(len(bboxes))
-            print(len(tokens))
-            os.exit()
-
-        assert len(bboxes) == len(tokens)
-        yield guid, {"id": str(guid), "tokens": tokens, "bboxes": bboxes, "ner_tags": ner_tags,
+                    # TODO: How did we endup with O-Token with size of [0,0,W,H]
+                    other_box  = normalize_bbox(w["box"], size)
+                    if other_box[0]== 0 and other_box[1] == 0:
+                        continue
+
+                    tokens.append(w["text"])
+                    ner_tags.append("O")
+                    cur_line_bboxes.append(other_box)
+            else:
+                tokens.append(words[0]["text"])
+                ner_tags.append("B-" + label.upper())
+                cur_line_bboxes.append(normalize_bbox(words[0]["box"], size))
+                for w in words[1:]:
+                    tokens.append(w["text"])
+                    ner_tags.append("I-" + label.upper())
+                    cur_line_bboxes.append(normalize_bbox(w["box"], size))
+
+            # by default: --segment_level_layout 1
+            # if do not want to use segment_level_layout, comment the following line
+            if len(cur_line_bboxes) == 0:
+                print(f"Empty cur_line_bboxes for {words} : {file_path}")
+                continue
+                
+            cur_line_bboxes = self.get_line_bbox(cur_line_bboxes)
+            bboxes.extend(cur_line_bboxes)
+
+
+        if len(bboxes) == 0:
+            payload = {"id": str(guid), "tokens": tokens, "bboxes": bboxes, "ner_tags": ner_tags}
+            print(f"Empty Boxes for : {file_path}")
+            return 
+
+        return guid, {"id": str(guid), "tokens": tokens, "bboxes": bboxes, "ner_tags": ner_tags,
                         "image": image, "image_path": image_path}
 
 
-
-    def _generate_examplesXXXX(self, filepath):
+    def _generate_examples(self, filepath):
         logger.info("⏳ Generating examples from = %s", filepath)
         ann_dir = os.path.join(filepath, "annotations")
         img_dir = os.path.join(filepath, "images")
@@ -271,26 +269,26 @@ class Funsd(datasets.GeneratorBasedBuilder):
         items = sorted(os.listdir(ann_dir))
         np.random.shuffle(items)
 
-        stop = int(len(items) *.1)
+        stop = int(len(items) *.50)
         stop = int(len(items))
         
-        print(len(items))
+        # https://stackoverflow.com/questions/47776486/python-struct-error-i-format-requires-2147483648-number-2147483647
+        self.filepath = filepath
         for guid, file in enumerate(items):
             file_path = os.path.join(ann_dir, file)
-            __args = (filepath, guid, file)
+            __args = (guid, file)
             args.append(__args)
             if guid == stop:
                 break
 
         results = []
         start = time.time()
-        processes = int(mp.cpu_count() * .95)
-        processes = 4
-
-        print(f"\nPool Executor: {processes}")
+        print("\nPool Executor:")
         print("Time elapsed: %s" % (time.time() - start))
 
-        pool = Pool(processes=processes, maxtasksperchild=1024)
+        processes = int(mp.cpu_count() * .90)
+        processes = 4
+        pool = Pool(processes=processes)
         pool_results = pool.starmap(self._generate_, args)
 
         pool.close()
diff --git a/layoutlmv3/train.sh b/layoutlmv3/train.sh
index dc352c2..5e5c839 100755
--- a/layoutlmv3/train.sh
+++ b/layoutlmv3/train.sh
@@ -2,7 +2,7 @@
 
 export BS=16
 export MAX_JOBS=8
-# CUDA_VISIBLE_DEVICES=1 
+CUDA_VISIBLE_DEVICES=1 
 
 # --max_train_samples 5000 \
 # --max_test_samples 1000 \
@@ -28,13 +28,12 @@ export MAX_JOBS=8
     # --resume_from_checkpoint /mnt/data/models/layoutlmv3-base-finetuned-segment_level_layout/checkpoint-500 \
 
 
-PYTHONPATH="$PWD" python -m torch.distributed.launch \
-    --nproc_per_node=2 --master_port 4398 examples/run_funsd.py \
+PYTHONPATH="$PWD" python  examples/run_funsd.py \
     --dataset_name funsd \
     --do_train \
     --do_eval \
     --model_name_or_path microsoft/layoutlmv3-base \
-    --output_dir /home/gbugaj/tmp/models/layoutlmv3-base-finetuned \
+    --output_dir /mnt/data/models/layoutlmv3-base-finetuned \
     --segment_level_layout 1 --visual_embed 1 --input_size 224 \
     --max_steps 10000 \
     --save_steps 500 \
@@ -45,9 +44,9 @@ PYTHONPATH="$PWD" python -m torch.distributed.launch \
     --per_device_train_batch_size 8 \
     --gradient_accumulation_steps 1 \
     --return_entity_level_metrics false \
-    --dataloader_num_workers 8 \
+    --dataloader_num_workers 1 \
     --cache_dir /tmp/cache/ \
-    --preprocessing_num_workers 8 \
+    --preprocessing_num_workers 1 \
     --overwrite_output_dir \
     --pad_to_max_length true \
     --fp16
