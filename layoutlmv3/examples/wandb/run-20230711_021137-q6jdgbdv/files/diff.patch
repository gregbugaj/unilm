diff --git a/layoutlmv3/README-GB.md b/layoutlmv3/README-GB.md
index bd4f519..d2f16fa 100644
--- a/layoutlmv3/README-GB.md
+++ b/layoutlmv3/README-GB.md
@@ -21,11 +21,17 @@ python -m torch.distributed.launch --nproc_per_node=1 ./train.py
 ``` 
 
 
-
 ```
 tensorboard --logdir=./logs
 ```
 
+```
+wandb server start --upgrade
+```
+
+```
+wandb login --relogin
+```
 
 
 LARGE
@@ -248,6 +254,114 @@ LARGE
 
 
 
+-- BASE 
+wandb: Run summary:
+wandb:                  eval/accuracy 0.89267
+wandb:                        eval/f1 0.84425
+wandb:                      eval/loss 1.22023
+wandb:                 eval/precision 0.83448
+wandb:                    eval/recall 0.85426
+wandb:                   eval/runtime 8.3803
+wandb:        eval/samples_per_second 47.134
+wandb:          eval/steps_per_second 47.134
+wandb:                    train/epoch 50.25
+wandb:              train/global_step 10000
+wandb:            train/learning_rate 0.0
+wandb:                     train/loss 0.0001
+wandb:               train/total_flos 1.0610876067072e+16
+wandb:               train/train_loss 0.04689
+wandb:            train/train_runtime 1368.8808
+wandb: train/train_samples_per_second 29.221
+wandb:   train/train_steps_per_second 7.305
+
+
+
+--- BASE SEGMENT_CURRENT_LINE
+
+wandb: 
+wandb: Run history:
+wandb:                  eval/accuracy ▅▄▁▄▇▆▆▅██▇
+wandb:                        eval/f1 ▄▅▁▄█▆▄▃▅▅█
+wandb:                      eval/loss ▁▄▅▆▇▅▆▇▇█▇
+wandb:                 eval/precision ▄▅▁▅█▆▃▁▄▅█
+wandb:                    eval/recall ▄▅▁▄█▇▆▅▆▆█
+wandb:                   eval/runtime ▆█▇▁▅▆▃▃▅▄▄
+wandb:        eval/samples_per_second ▂▁▂█▃▃▅▅▃▅▅
+wandb:          eval/steps_per_second ▂▁▂█▃▃▅▅▃▅▅
+wandb:                    train/epoch ▁▁▁▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇█████
+wandb:              train/global_step ▁▁▁▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇█████
+wandb:            train/learning_rate ██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁
+wandb:                     train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
+wandb:               train/total_flos ▁
+wandb:               train/train_loss ▁
+wandb:            train/train_runtime ▁
+wandb: train/train_samples_per_second ▁
+wandb:   train/train_steps_per_second ▁
+wandb: 
+wandb: Run summary:
+wandb:                  eval/accuracy 0.9023
+wandb:                        eval/f1 0.86765
+wandb:                      eval/loss 1.1141
+wandb:                 eval/precision 0.85966
+wandb:                    eval/recall 0.87579
+wandb:                   eval/runtime 9.6308
+wandb:        eval/samples_per_second 41.014
+wandb:          eval/steps_per_second 41.014
+wandb:                    train/epoch 50.25
+wandb:              train/global_step 10000
+wandb:            train/learning_rate 0.0
+wandb:                     train/loss 0.0001
+wandb:               train/total_flos 1.0610876067072e+16
+wandb:               train/train_loss 0.0379
+wandb:            train/train_runtime 1663.565
+wandb: train/train_samples_per_second 24.045
+wandb:   train/train_steps_per_second 6.011
+
+
+-- BASE  STRIDE 128
+
+wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
+wandb: 
+wandb: Run history:
+wandb:                  eval/accuracy ▁▆▆▅▅█▇▇▇▇▇
+wandb:                        eval/f1 ▁▆▆▆▇▇█▇███
+wandb:                      eval/loss ▁▁▃▅▇▅▆▆▇█▆
+wandb:                 eval/precision ▁▆▅▆▇▇█▇███
+wandb:                    eval/recall ▁▇▇▆▇▇█████
+wandb:                   eval/runtime ▅▅▇▄█▄▁▃▁▁▂
+wandb:        eval/samples_per_second ▄▄▂▄▁▅█▆██▇
+wandb:          eval/steps_per_second ▄▄▂▄▁▅█▆██▇
+wandb:                    train/epoch ▁▁▁▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇█████
+wandb:              train/global_step ▁▁▁▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇█████
+wandb:            train/learning_rate ██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁
+wandb:                     train/loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
+wandb:               train/total_flos ▁
+wandb:               train/train_loss ▁
+wandb:            train/train_runtime ▁
+wandb: train/train_samples_per_second ▁
+wandb:   train/train_steps_per_second ▁
+wandb: 
+wandb: Run summary:
+wandb:                  eval/accuracy 0.91079
+wandb:                        eval/f1 0.85973
+wandb:                      eval/loss 0.91185
+wandb:                 eval/precision 0.84622
+wandb:                    eval/recall 0.87368
+wandb:                   eval/runtime 13.3387
+wandb:        eval/samples_per_second 46.256
+wandb:          eval/steps_per_second 46.256
+wandb:                    train/epoch 30.3
+wandb:              train/global_step 10000
+wandb:            train/learning_rate 0.0
+wandb:                     train/loss 0.0001
+wandb:               train/total_flos 1.06002519108096e+16
+wandb:               train/train_loss 0.04648
+wandb:            train/train_runtime 1544.4472
+wandb: train/train_samples_per_second 25.899
+wandb:   train/train_steps_per_second 6.475
+
+
+
 # Reference 
 
 [Max_seq_length LayoutLMV3 - How to implement it ? #942](https://github.com/microsoft/unilm/issues/942)
diff --git a/layoutlmv3/examples/funsd_dataset/funsd_dataset.py b/layoutlmv3/examples/funsd_dataset/funsd_dataset.py
index 6b250a5..28075d3 100644
--- a/layoutlmv3/examples/funsd_dataset/funsd_dataset.py
+++ b/layoutlmv3/examples/funsd_dataset/funsd_dataset.py
@@ -153,6 +153,8 @@ class Funsd(datasets.GeneratorBasedBuilder):
         downloaded_file = "/data/dataset/private/corr-indexer-augmented"
         # downloaded_file = "/home/greg/dataset/assets-private/corr-indexer-augmented"
         downloaded_file = "/home/gbugaj/datasets/private/corr-indexer-augmented"
+        downloaded_file = "/home/greg/datasets/private/assets-private/corr-indexer-augmented"
+        # downloaded_file = "/home/gbugaj/dataset/private/corr-indexer-augmented"
 
         return [
             datasets.SplitGenerator(
@@ -179,7 +181,6 @@ class Funsd(datasets.GeneratorBasedBuilder):
         ann_dir = os.path.join(self.filepath, "annotations")
         img_dir = os.path.join(self.filepath, "images")
 
-        print(guid)
         tokens = []
         bboxes = []
         ner_tags = []
@@ -191,6 +192,22 @@ class Funsd(datasets.GeneratorBasedBuilder):
         image_path = os.path.join(img_dir, file)
         image_path = image_path.replace("json", "png")
         image, size = load_image(image_path)
+
+        # somehow we got a TEXT token with size of [0,0,W,H]
+        # TODO: investigate
+        # example :/corr-indexer-augmented/dataset/training_data/annotations/152658473_2_140_0.json
+        #       "words": [
+        # {
+        #   "box": [
+        #     0,
+        #     0,
+        #     776,
+        #     1000
+        #   ],
+        #   "text": ":"
+        # }
+        # ]
+    
         for item in data["form"]:
             cur_line_bboxes = []
             words, label = item["words"], item["label"]
@@ -202,6 +219,7 @@ class Funsd(datasets.GeneratorBasedBuilder):
             words = [w for w in words if w["text"].strip() != ""]
             if len(words) == 0:
                 continue
+
             if label == "other":
                 for w in words:
                     # TODO: How did we endup with O-Token with size of [0,0,W,H]
@@ -220,26 +238,20 @@ class Funsd(datasets.GeneratorBasedBuilder):
                     tokens.append(w["text"])
                     ner_tags.append("I-" + label.upper())
                     cur_line_bboxes.append(normalize_bbox(w["box"], size))
-            # cur_line_bboxes = self.get_line_bbox(cur_line_bboxes)
-
-            if False:
-                boxed = True
-                label = label.upper()
-                if label == "OTHER" or label== "ANSWER" or label == "PARAGRAPH" or label == "ADDRESS" or label == "GREETING" or label == "HEADER" :
-                    boxed = False
-
-                if boxed:
-                    # print(f"B {label} : {words} >> {cur_line_bboxes}")
-                    cur_line_bboxes = self.get_line_bbox(cur_line_bboxes)
-                    pass
-                    
+
+            # by default: --segment_level_layout 1
+            # if do not want to use segment_level_layout, comment the following line
+            if len(cur_line_bboxes) == 0:
+                print(f"Empty cur_line_bboxes for {words} : {file_path}")
+                continue
+                
+            cur_line_bboxes = self.get_line_bbox(cur_line_bboxes)
             bboxes.extend(cur_line_bboxes)
 
 
         if len(bboxes) == 0:
             payload = {"id": str(guid), "tokens": tokens, "bboxes": bboxes, "ner_tags": ner_tags}
             print(f"Empty Boxes for : {file_path}")
-            # print(payload)
             return 
 
         return guid, {"id": str(guid), "tokens": tokens, "bboxes": bboxes, "ner_tags": ner_tags,
diff --git a/layoutlmv3/examples/train.py b/layoutlmv3/examples/train.py
index c41ad25..d04a6b4 100644
--- a/layoutlmv3/examples/train.py
+++ b/layoutlmv3/examples/train.py
@@ -80,6 +80,7 @@ if False:
 # processor = AutoProcessor.from_pretrained("microsoft/layoutlmv3-base", apply_ocr=False)
 
 model_name_or_path = "microsoft/layoutlmv3-large"
+# model_name_or_path = "microsoft/layoutlmv3-base"
 
 config = AutoConfig.from_pretrained (
     model_name_or_path,
@@ -87,9 +88,9 @@ config = AutoConfig.from_pretrained (
     finetuning_task="ner",
     cache_dir="/mnt/data/cache",
     input_size=224,
-    hidden_dropout_prob = .2,
-    attention_probs_dropout_prob = .2,
-    has_relative_attention_bias=False
+    # hidden_dropout_prob = .1,
+    # attention_probs_dropout_prob = .1,
+    # has_relative_attention_bias=False
 )
 
 # config.hidden_dropout_prob = 0.2
@@ -140,7 +141,14 @@ def prepare_examples(examples):
   boxes = examples[boxes_column_name]
   word_labels = examples[label_column_name]
 
-  encoding = processor(images, words, boxes=boxes, word_labels=word_labels, truncation=True,  padding="max_length")
+#   encoding = processor(images, words, boxes=boxes, word_labels=word_labels, truncation=True,  padding="max_length")
+
+  encoding = processor(images, words, boxes=boxes, word_labels=word_labels, truncation=True, stride =128, 
+         padding="max_length", max_length=512, return_overflowing_tokens=True, return_offsets_mapping=True)  
+  
+  offset_mapping = encoding.pop('offset_mapping')
+  overflow_to_sample_mapping = encoding.pop('overflow_to_sample_mapping')
+  
   return encoding
 
 # we need to define custom features for `set_format` (used later on) to work properly
@@ -239,8 +247,8 @@ training_args = TrainingArguments(
                   eval_steps=1000,
                   load_best_model_at_end=True,
                   metric_for_best_model="f1",
-                  output_dir="/mnt/data/models/layoutlmv3-large-fullyannotated",
-                  # resume_from_checkpoint="/mnt/data/models/layoutlmv3-large-finetuned-small-100/checkpoint-750",
+                  output_dir="/mnt/data/models/layoutlmv3-large-stride",
+                  resume_from_checkpoint="/mnt/data/models/layoutlmv3-large-stride/checkpoint-2000",
                   fp16=True,
                 )
 
@@ -405,26 +413,3 @@ for word, box, label in zip(example['tokens'], example['bboxes'], example['ner_t
 image.save("/tmp/tensors/real.png")
 
 
-#
-#
-# Loading best model from /mnt/data/models/layoutlmv3-large-finetuned-small/checkpoint-4500 (score: 0.9116538131962296).
-# {'train_runtime': 2593.4786, 'train_samples_per_second': 7.712, 'train_steps_per_second': 1.928, 'train_loss': 0.06096109193563461, 'epoch': 18.73}
-# 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [43:13<00:00,  1.93it/s]
-# Saving model checkpoint to /mnt/data/models/layoutlmv3-large-finetuned-small
-# Configuration saved in /mnt/data/models/layoutlmv3-large-finetuned-small/config.json
-# Model weights saved in /mnt/data/models/layoutlmv3-large-finetuned-small/pytorch_model.bin
-# Feature extractor saved in /mnt/data/models/layoutlmv3-large-finetuned-small/preprocessor_config.json
-# tokenizer config file saved in /mnt/data/models/layoutlmv3-large-finetuned-small/tokenizer_config.json
-# Special tokens file saved in /mnt/data/models/layoutlmv3-large-finetuned-small/special_tokens_map.json
-# ***** train metrics *****
-#   epoch                    =      18.73
-#   train_loss               =      0.061
-#   train_runtime            = 0:43:13.47
-#   train_samples            =       1065
-#   train_samples_per_second =      7.712
-#   train_steps_per_second   =      1.928
-# ***** Running Evaluation *****
-#   Num examples = 400
-#   Batch size = 1
-
-
